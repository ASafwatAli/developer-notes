(window.webpackJsonp=window.webpackJsonp||[]).push([[136],{"./manual/Machine-Learning/ML-Decision-Trees.md":function(e,n,t){"use strict";t.r(n),t.d(n,"default",function(){return h});var a=t("./node_modules/react/index.js"),o=t.n(a),r=t("./node_modules/@mdx-js/tag/dist/index.js");function s(e){return(s="function"===typeof Symbol&&"symbol"===typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"===typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function i(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}function m(e,n){for(var t=0;t<n.length;t++){var a=n[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(e,a.key,a)}}function c(e,n){return!n||"object"!==s(n)&&"function"!==typeof n?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):n}function p(e){return(p=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function l(e,n){return(l=Object.setPrototypeOf||function(e,n){return e.__proto__=n,e})(e,n)}var h=function(e){function n(e){var t;return function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,n),(t=c(this,p(n).call(this,e))).layout=null,t}var t,a,s;return function(e,n){if("function"!==typeof n&&null!==n)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(n&&n.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),n&&l(e,n)}(n,o.a.Component),t=n,(a=[{key:"render",value:function(){var e=this.props,n=e.components;i(e,["components"]);return o.a.createElement(r.MDXTag,{name:"wrapper",components:n},o.a.createElement(r.MDXTag,{name:"h1",components:n,props:{id:"decision-trees"}},"Decision Trees"),o.a.createElement(r.MDXTag,{name:"ul",components:n},o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#decision-trees"}},"Decision Trees"),o.a.createElement(r.MDXTag,{name:"ul",components:n,parentName:"li"},o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#intuition"}},"Intuition")),o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#decision-tree-regression-in-python"}},"Decision Tree Regression in Python"))))),o.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"intuition"}},"Intuition"),o.a.createElement(r.MDXTag,{name:"p",components:n},o.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"CART: Classification and Regression Trees")),o.a.createElement(r.MDXTag,{name:"p",components:n},"We speak about both types, but for now - focus on regression trees."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Regression trees are a bit more complex than classification trees."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Imagine a scatter plot with two IV and we are predicting an DV y (which you wouldn't be able to see on the chart). Essentially the DV would sit on the z axis."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Once you run the regression decision tree algorithm, the scatter plot will be split up into segments."),o.a.createElement(r.MDXTag,{name:"p",components:n},"For example, x1 might be split at 20. Another split may happen for x2 at 170, 200 etc."),o.a.createElement(r.MDXTag,{name:"p",components:n},"The question, are the splits adding value to way we want to group our points?"),o.a.createElement(r.MDXTag,{name:"p",components:n},"Each split itself is known as a leaf."),o.a.createElement(r.MDXTag,{name:"p",components:n},"The algorithm can handle mathematical issues and we can focus on the practical element of the algorithm."),o.a.createElement(r.MDXTag,{name:"p",components:n},o.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"Splitting")),o.a.createElement(r.MDXTag,{name:"p",components:n},"If we split ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[1] < 20"),", we have two options (y/N). If we then split ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[2] < 170"),", we add a child node to ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[1] < 20")," that checks y/N. If we then set ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},""),"x","[2]"," < 200`."),o.a.createElement(r.MDXTag,{name:"p",components:n},"After having a two child tree, if we set ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[1] < 40")," such that ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[1] < 20")," is not true and ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[2] < 170")," is true, we can then set ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[1] < 40")," as the child to ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"x[2] < 170"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Once we start this tree, what do we populate into those boxes? Well, we decide how we predict ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"y")," with a new observation added to the plane x","[1]"," and x","[2]","."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Key note: ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"Adding splits adds information"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"What we do is that for each terminal leaf, we take the average and assign the value that we give to any new element that falls into that leaf."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Now, if we have a new value, we check the decision tree where it falls and then assign the new element the value of where it falls as a prediction."),o.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"decision-tree-regression-in-python"}},"Decision Tree Regression in Python"),o.a.createElement(r.MDXTag,{name:"p",components:n},"Warning for the decision tree, because we need to consider the entropy and split the result into data points. If we stick to one dimension, how do we have a line that is not horizontal? If the splits are made, they should remain a constant."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Either the intervals are infinite (which they are not), or the model has an issue."),o.a.createElement(r.MDXTag,{name:"p",components:n},"The reason the issue came up, is because of what we have used to create the plot since this is no longer linear."),o.a.createElement(r.MDXTag,{name:"p",components:n},"This is now a non-linear, non-continuous regression model."),o.a.createElement(r.MDXTag,{name:"p",components:n},"What is the best way to view something non-continuous?"),o.a.createElement(r.MDXTag,{name:"pre",components:n},o.a.createElement(r.MDXTag,{name:"code",components:n,parentName:"pre",props:{className:"language-python"}},"# Visualising the Decision Tree results\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape(len(X_grid), 1)\nplt.scatter(X, y, color = 'red')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\nplt.title('Truth or Bluff (Decision Tree Regression)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.savefig('decision-tree.png')\nplt.show()\n")),o.a.createElement(r.MDXTag,{name:"p",components:n},"As for getting the decision tree code to run:"),o.a.createElement(r.MDXTag,{name:"pre",components:n},o.a.createElement(r.MDXTag,{name:"code",components:n,parentName:"pre",props:{className:"language-python"}},"# Prediciting the Decision Tree results\n# Create the Regressor\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=0)\nregressor.fit(X, y)\n\ny_pred = regressor.predict(6.5)\n")),o.a.createElement(r.MDXTag,{name:"p",components:n},"Ensure you have a higher resolution in order to visualize the splits. Given that the example in the tutorial has just 1 DV and 1 IV, it will come out like steps as the only splits will occur on the x axis."),o.a.createElement(r.MDXTag,{name:"p",components:n},"The model itself is not necessarily that interesting in 1D, but over many dimensions it becomes far more interesting."),o.a.createElement(r.MDXTag,{name:"p",components:n},o.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"What happens when you use a random forest?")),o.a.createElement(r.MDXTag,{name:"p",components:n},"A Random Forest is a team of decision trees. What happens with a team of 10 trees? 50 trees? 500 trees?"))}}])&&m(t.prototype,a),s&&m(t,s),n}();h.__docgenInfo={description:"",methods:[],displayName:"MDXContent"}}}]);
//# sourceMappingURL=manual-machine-learning-ml-decision-trees.305817e8304de77c75ce.js.map