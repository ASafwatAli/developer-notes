(window.webpackJsonp=window.webpackJsonp||[]).push([[137],{"./manual/Machine-Learning/ML-Random-Forest-Regression.md":function(e,n,t){"use strict";t.r(n),t.d(n,"default",function(){return u});var o=t("./node_modules/react/index.js"),a=t.n(o),r=t("./node_modules/@mdx-js/tag/dist/index.js");function s(e){return(s="function"===typeof Symbol&&"symbol"===typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"===typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function i(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}function m(e,n){for(var t=0;t<n.length;t++){var o=n[t];o.enumerable=o.enumerable||!1,o.configurable=!0,"value"in o&&(o.writable=!0),Object.defineProperty(e,o.key,o)}}function c(e,n){return!n||"object"!==s(n)&&"function"!==typeof n?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):n}function l(e){return(l=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function p(e,n){return(p=Object.setPrototypeOf||function(e,n){return e.__proto__=n,e})(e,n)}var u=function(e){function n(e){var t;return function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,n),(t=c(this,l(n).call(this,e))).layout=null,t}var t,o,s;return function(e,n){if("function"!==typeof n&&null!==n)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(n&&n.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),n&&p(e,n)}(n,a.a.Component),t=n,(o=[{key:"render",value:function(){var e=this.props,n=e.components;i(e,["components"]);return a.a.createElement(r.MDXTag,{name:"wrapper",components:n},a.a.createElement(r.MDXTag,{name:"h1",components:n,props:{id:"random-forest-regression"}},"Random Forest Regression"),a.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"intuition"}},"Intuition"),a.a.createElement(r.MDXTag,{name:"p",components:n},"Random forest is a version of ensemble learning."),a.a.createElement(r.MDXTag,{name:"p",components:n},"It's when you take the same algorithm multiple times and create something more powerful."),a.a.createElement(r.MDXTag,{name:"p",components:n},a.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"Steps")),a.a.createElement(r.MDXTag,{name:"ol",components:n},a.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ol"},"Pick at random K data points from the Training Set."),a.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ol"},"Build the Decision Tree associated to these K data points."),a.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ol"},"Choose the number Ntree of trees you want to build and repeat steps 1 and 2."),a.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ol"},"For a new data point, make each one of your Ntree trees predict the value of ",a.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"li"},"Y")," for the data point in question, and assign the new data point the average across all the predicted ",a.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"li"},"Y")," values.")),a.a.createElement(r.MDXTag,{name:"p",components:n},"Doing this allows you to improve the accuracy of your prediction."),a.a.createElement(r.MDXTag,{name:"p",components:n},a.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"Example")),a.a.createElement(r.MDXTag,{name:"p",components:n},"How many lollies in a jar? Imagine taking notes of every guess - getting around 1000 and then beginning to average them out or take the median. Statistically speaking, you have a highly likelihood of being closer to the truth."),a.a.createElement(r.MDXTag,{name:"p",components:n},"Once you hit the middle of the normal distribution, you are more likely to be on the money for the guess."),a.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"python"}},"PYTHON"),a.a.createElement(r.MDXTag,{name:"p",components:n},"This is the last regression model. If you understand decision tree regression, you'll understand random forest."),a.a.createElement(r.MDXTag,{name:"p",components:n},"From decision tree, we know that we will need the visualisation using the non-continuous result."),a.a.createElement(r.MDXTag,{name:"p",components:n},"For the regressor, we use RandomForestRegressor library."),a.a.createElement(r.MDXTag,{name:"pre",components:n},a.a.createElement(r.MDXTag,{name:"code",components:n,parentName:"pre",props:{className:"language-python"}},"# Prediciting the Random Forest results\n# Create the Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(random_state=0)\nregressor.fit(X, y)\n")),a.a.createElement(r.MDXTag,{name:"p",components:n},"Simply, with these lines, we can already determine that the graph is no longer continuous."),a.a.createElement(r.MDXTag,{name:"p",components:n},'By having several decision trees, we end up with a lot more "steps" than we had with just one decision tree.'),a.a.createElement(r.MDXTag,{name:"p",components:n},"More tree !== more steps. The more trees you have, the more the average will converge towards the same average."),a.a.createElement(r.MDXTag,{name:"p",components:n},"Generally the steps will become better placed depending on the average."))}}])&&m(t.prototype,o),s&&m(t,s),n}();u.__docgenInfo={description:"",methods:[],displayName:"MDXContent"}}}]);
//# sourceMappingURL=manual-machine-learning-ml-random-forest-regression.305817e8304de77c75ce.js.map