(window.webpackJsonp=window.webpackJsonp||[]).push([[138],{"./manual/Machine-Learning/ML-Regression-Model-Performance.md":function(e,n,t){"use strict";t.r(n),t.d(n,"default",function(){return u});var a=t("./node_modules/react/index.js"),o=t.n(a),r=t("./node_modules/@mdx-js/tag/dist/index.js");function i(e){return(i="function"===typeof Symbol&&"symbol"===typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"===typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function s(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}function m(e,n){for(var t=0;t<n.length;t++){var a=n[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(e,a.key,a)}}function l(e,n){return!n||"object"!==i(n)&&"function"!==typeof n?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):n}function c(e){return(c=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function p(e,n){return(p=Object.setPrototypeOf||function(e,n){return e.__proto__=n,e})(e,n)}var u=function(e){function n(e){var t;return function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,n),(t=l(this,c(n).call(this,e))).layout=null,t}var t,a,i;return function(e,n){if("function"!==typeof n&&null!==n)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(n&&n.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),n&&p(e,n)}(n,o.a.Component),t=n,(a=[{key:"render",value:function(){var e=this.props,n=e.components;s(e,["components"]);return o.a.createElement(r.MDXTag,{name:"wrapper",components:n},o.a.createElement(r.MDXTag,{name:"h1",components:n,props:{id:"regression-model-performance"}},"Regression Model Performance"),o.a.createElement(r.MDXTag,{name:"ul",components:n},o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#regression-model-performance"}},"Regression Model Performance"),o.a.createElement(r.MDXTag,{name:"ul",components:n,parentName:"li"},o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#r-squared-intuition"}},"R-Squared Intuition")),o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#adjusted-r-squared-intuition"}},"Adjusted R-Squared Intuition")),o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#evaluating-regression-models-performance"}},"Evaluating Regression Models Performance")),o.a.createElement(r.MDXTag,{name:"li",components:n,parentName:"ul"},o.a.createElement(r.MDXTag,{name:"a",components:n,parentName:"li",props:{href:"#interpretting-linear-regressions-coefficients"}},"Interpretting Linear Regressions Coefficients"))))),o.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"r-squared-intuition"}},"R-Squared Intuition"),o.a.createElement(r.MDXTag,{name:"p",components:n},"Interesting parameter and most people use it without understanding the underlying principles."),o.a.createElement(r.MDXTag,{name:"p",components:n},"We spoke about the ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"simple linear regression")," model being the result of the ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"ordinary least squares")," method. This is also known as ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"sum of squares residuals")," and is given as ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"SS[res]"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"If we instead took ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"SUM(y[i] - y[avg])^2"),", we get the ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"total sum of squares SS[tot]"),". To get our regression, we get ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"R^2 = 1 - (SS[res]/SS[tot])"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"As you minimize the SS","[res]",", it becomes smaller, and as we get ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"1 - (SS[res]/SS[tot])")," we actually start to get closer to 1. The closer we get to 1, the better. Can R^2 be negative? Yes. It can if the SS","[res]"," fits the line worse. In that case it would be better to use the average than the model - although it would be hard to do!"),o.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"adjusted-r-squared-intuition"}},"Adjusted R-Squared Intuition"),o.a.createElement(r.MDXTag,{name:"p",components:n},"This is the fun part!"),o.a.createElement(r.MDXTag,{name:"p",components:n},"Here we have our ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"simple linear regression")," regression from before. We, the same concepts apply for multiple linear regression."),o.a.createElement(r.MDXTag,{name:"p",components:n},"R^2 - goodness of fit. The closer the one, the better - BUT the problem is when we start adding more IVs into the model. What we can look at is whether the R^2 getting better or worse, but because of SS","[res]"," the minimum will never decrease. ",o.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"THIS IS IMPORTANT"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Once you add a new variable, it will affect what the variable looks like. Either the new variable will help minimize the SS","[res]",". If you cannot decress SS","[res]",", the new variable would be zero (unlikely)."),o.a.createElement(r.MDXTag,{name:"p",components:n},"Therefore, ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"R^2")," will never decrease when you add in more variables. That being said, if the IV has zero correlation or causation with the DV, there randomly will be a slight correlation - therefore R^2 might slightly increase even though the variable is not helping the model."),o.a.createElement(r.MDXTag,{name:"p",components:n},"This is where ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"adjusted R^2")," comes in."),o.a.createElement(r.MDXTag,{name:"pre",components:n},o.a.createElement(r.MDXTag,{name:"code",components:n,parentName:"pre",props:{}},"Adj R^2 = 1 - (1-R^2)*((n - 1)/(n - p - 1))\np - number of regressors\nn - sample size\n")),o.a.createElement(r.MDXTag,{name:"p",components:n},"This formula will penalise you for adding in IVs that have no correlation."),o.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"evaluating-regression-models-performance"}},"Evaluating Regression Models Performance"),o.a.createElement(r.MDXTag,{name:"p",components:n},"Reflecting back on the ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"Backward Elimination")," process that we used, we actually came to a feeling that we shouldn't have excluded the last value of ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"Marketing Spend"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"The problem with the algorithms that we chose and the p value threshhold we chose is that the threshhold is arbitrary."),o.a.createElement(r.MDXTag,{name:"p",components:n},"What we can actually find at the bottom of those same reports that we used is that it also gives us the values of ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"R-squared")," and ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"Adjusted R-squared"),"."),o.a.createElement(r.MDXTag,{name:"p",components:n},"If we observe this as we build our model in code, then we can use the report to help check the goodness of fit."),o.a.createElement(r.MDXTag,{name:"h2",components:n,props:{id:"interpretting-linear-regressions-coefficients"}},"Interpretting Linear Regressions Coefficients"),o.a.createElement(r.MDXTag,{name:"p",components:n},"If we look at the third model and look at the IVs. If we change the IV value, we should be able to tell that DV should correlate in a certain direction."),o.a.createElement(r.MDXTag,{name:"p",components:n},"The coefficients come under their own heading."),o.a.createElement(r.MDXTag,{name:"p",components:n},"We should be able to ue logistics just for this. Magnitude can also trip you up. The ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"Estimate")," value doesn't really give you more indication. We could alter the value of input and it would change values for ",o.a.createElement(r.MDXTag,{name:"inlineCode",components:n,parentName:"p"},"Estimate")," etc. Think of magnitude in term of units of the IV."),o.a.createElement(r.MDXTag,{name:"p",components:n},"What the estimate shows that for every unit you increase, the IV would increase by a certain amount."),o.a.createElement(r.MDXTag,{name:"p",components:n},o.a.createElement(r.MDXTag,{name:"strong",components:n,parentName:"p"},"THE IMPORTANT PART: THING IN TERMS OF PER UNIT")),o.a.createElement(r.MDXTag,{name:"p",components:n},"You'll also know that as you take out certain IVs, the actual effect that it brings into the model will also change."))}}])&&m(t.prototype,a),i&&m(t,i),n}();u.__docgenInfo={description:"",methods:[],displayName:"MDXContent"}}}]);
//# sourceMappingURL=manual-machine-learning-ml-regression-model-performance.305817e8304de77c75ce.js.map